# **Hi, I'm Samuel ğŸ‘‹**  

I'm a **Masterâ€™s candidate in Computer Science & Mathematics** with a passion for **building scalable data solutions, optimizing workflows, and leveraging AI-driven insights**. I enjoy working at the intersection of **data, technology, and business**, developing systems that turn raw information into **actionable intelligence**.  

---

## ğŸ“‚ **Projects I've Worked On**  

### ğŸ”¹ **Automated ETL & Data Pipelines**  
Developed **Python & SQL-based ETL workflows** to streamline data ingestion, reducing processing time by **40%** and improving data accuracy. Integrated **AWS Glue, Lambda, and S3** for scalable, cloud-based data management.  

### ğŸ”¹ **Predictive Modeling for Credit Risk**  
Built **XGBoost & LightGBM** models to assess credit risk, achieving an **AUC-ROC of 0.92**. Applied **feature engineering, hyperparameter tuning, and model validation techniques** to improve classification accuracy.  

### ğŸ”¹ **Business Intelligence & Visualization**  
Designed **interactive Power BI dashboards** to provide real-time analytics, improving decision-making and increasing engagement by **20%**.  

---

## ğŸš€ **What Iâ€™m Currently Working On**  

### ğŸï¸ **F1 Strategic Intelligence Platform (F1-SIP) ğŸ“Š**  
A cloud-native platform that **processes real-time F1 race telemetry data** to generate **AI-powered strategic insights** for race teams. It enables data-driven decision-making by analyzing **historical race performance, weather conditions, and car telemetry metrics** to optimize race strategies.  

### **Key Technologies & Features**  
âœ” **Streaming Data Pipelines** â€“ Built with **Apache Kafka** to process live telemetry data at scale.  
âœ” **Data Quality & Validation** â€“ Ensuring integrity using **Great Expectations & dbt**.  
âœ” **Scalable ETL & Data Modeling** â€“ Optimizing **SQL & dbt transformations** for analytics.  
âœ” **Infrastructure as Code (IaC)** â€“ Automating deployments with **Terraform & Kubernetes**.  
âœ” **Performance & Cost Optimization** â€“ Leveraging **serverless architecture** for efficiency.  

ğŸ”— **Project Repo:** [GitHub Repository](https://github.com/your-repo-link)  

ğŸ“Œ **Code Sample:** *(Example of a streaming pipeline using Kafka & Python)*  

```python
from kafka import KafkaConsumer

consumer = KafkaConsumer(
    'f1_telemetry_data',
    bootstrap_servers=['kafka-broker:9092'],
    auto_offset_reset='earliest',
    enable_auto_commit=True,
    group_id='f1-analytics-group'
)

for message in consumer:
    process_data(message.value)  # Custom function to handle incoming telemetry data
```

## ğŸ¯ **Opportunities & Collaboration**  

Iâ€™m actively seeking **internship and entry-level opportunities** in **data engineering, analytics, and cloud computing**.  
Iâ€™m particularly interested in roles that involve **scalable data pipelines, machine learning for predictive analytics, and cloud-based data architecture**.  

Iâ€™m also open to **collaborations, open-source contributions, and discussing innovative data solutions** in **FinTech, AI, and sports analytics**.  
If youâ€™re working on **cutting-edge data projects**, letâ€™s connect!  

ğŸ“© **Find me on:**  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/samuelvurity/)  
ğŸ“§ [Email](mailto:your-email@example.com)  
