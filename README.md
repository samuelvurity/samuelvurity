# **Hi, I'm Samuel 👋**  

I'm a **Master’s candidate in Computer Science & Mathematics** with a passion for **building scalable data solutions, optimizing workflows, and leveraging AI-driven insights**. I enjoy working at the intersection of **data, technology, and business**, developing systems that turn raw information into **actionable intelligence**.  

---

## 📂 **Projects I've Worked On**  

### 🔹 **Automated ETL & Data Pipelines**  
Developed **Python & SQL-based ETL workflows** to streamline data ingestion, reducing processing time by **40%** and improving data accuracy. Integrated **AWS Glue, Lambda, and S3** for scalable, cloud-based data management.  

### 🔹 **Predictive Modeling for Credit Risk**  
Built **XGBoost & LightGBM** models to assess credit risk, achieving an **AUC-ROC of 0.92**. Applied **feature engineering, hyperparameter tuning, and model validation techniques** to improve classification accuracy.  

### 🔹 **Business Intelligence & Visualization**  
Designed **interactive Power BI dashboards** to provide real-time analytics, improving decision-making and increasing engagement by **20%**.  

---

## 🚀 **What I’m Currently Working On**  

### 🏎️ **F1 Strategic Intelligence Platform (F1-SIP) 📊**  
A cloud-native platform that **processes real-time F1 race telemetry data** to generate **AI-powered strategic insights** for race teams. It enables data-driven decision-making by analyzing **historical race performance, weather conditions, and car telemetry metrics** to optimize race strategies.  

### **Key Technologies & Features**  
✔ **Streaming Data Pipelines** – Built with **Apache Kafka** to process live telemetry data at scale.  
✔ **Data Quality & Validation** – Ensuring integrity using **Great Expectations & dbt**.  
✔ **Scalable ETL & Data Modeling** – Optimizing **SQL & dbt transformations** for analytics.  
✔ **Infrastructure as Code (IaC)** – Automating deployments with **Terraform & Kubernetes**.  
✔ **Performance & Cost Optimization** – Leveraging **serverless architecture** for efficiency.  

🔗 **Project Repo:** [GitHub Repository](https://github.com/your-repo-link)  

📌 **Code Sample:** *(Example of a streaming pipeline using Kafka & Python)*  

```python
from kafka import KafkaConsumer

consumer = KafkaConsumer(
    'f1_telemetry_data',
    bootstrap_servers=['kafka-broker:9092'],
    auto_offset_reset='earliest',
    enable_auto_commit=True,
    group_id='f1-analytics-group'
)

for message in consumer:
    process_data(message.value)  # Custom function to handle incoming telemetry data
```

## 🎯 **Opportunities & Collaboration**  

I’m actively seeking **internship and entry-level opportunities** in **data engineering, analytics, and cloud computing**.  
I’m particularly interested in roles that involve **scalable data pipelines, machine learning for predictive analytics, and cloud-based data architecture**.  

I’m also open to **collaborations, open-source contributions, and discussing innovative data solutions** in **FinTech, AI, and sports analytics**.  
If you’re working on **cutting-edge data projects**, let’s connect!  

📩 **Find me on:**  
🔗 [LinkedIn](https://www.linkedin.com/in/samuelvurity/)  
📧 [Email](mailto:your-email@example.com)  
